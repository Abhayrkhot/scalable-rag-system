name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Lint with ruff
      run: |
        ruff check app/
        black --check app/
    
    - name: Type check with mypy
      run: |
        mypy app/
    
    - name: Test with pytest
      run: |
        pytest tests/ -v --cov=app --cov-report=xml
    
    - name: Security scan with bandit
      run: |
        bandit -r app/
    
    - name: Dependency check with safety
      run: |
        safety check
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  evaluation:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run evaluation suite
      run: |
        python run_evaluation.py
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    
    - name: Check evaluation results
      run: |
        if [ -f evaluation_results.json ]; then
          python -c "
          import json
          with open('evaluation_results.json') as f:
              results = json.load(f)
          if 'ragas' in results:
              overall_score = results['ragas'].get('overall_score', 0)
              if overall_score < 0.7:
                  print(f'Evaluation score too low: {overall_score}')
                  exit(1)
              print(f'Evaluation score: {overall_score}')
          "
        else:
          echo "No evaluation results found"
          exit 1
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-results
        path: evaluation_results.json

  build:
    runs-on: ubuntu-latest
    needs: [test, evaluation]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Build Docker image
      run: |
        docker build -t rag-system:latest .
    
    - name: Test Docker image
      run: |
        docker run --rm rag-system:latest python -c "import app; print('Import successful')"
    
    - name: Push to registry
      if: github.ref == 'refs/heads/main'
      run: |
        echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker tag rag-system:latest ${{ secrets.DOCKER_USERNAME }}/rag-system:latest
        docker push ${{ secrets.DOCKER_USERNAME }}/rag-system:latest

  deploy:
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        echo "Deployment would happen here"
        # Add actual deployment commands
